\documentclass{article}
\usepackage{ijcai17}

% Use the postscript times font!
\usepackage{times}

% the following package is optional:
%\usepackage{latexsym} 

% MIRI Default Packages:
\usepackage[citefix,noobjects,nofunctions]{miritools}
\setcounter{secnumdepth}{1}

%PAPER-SPECIFIC PACKAGES:
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amsthm} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amsopn}

\usepackage{pgfplots}
%Comment out when submitting
%\pgfplotsset{compat=1.14}
\usepackage{abbrevs}

\usepackage{multirow}

\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\usepackage{color}%multicol,multirow,tabularx,color}
%\definecolor{envNodeColor}{cmyk}{0.7,0.7,0.2,0}
\definecolor{envNodeColor}{rgb}{0.6,0.6,0.9}
\definecolor{envArrowColor}{rgb}{0.2,0.2,0.8}
\definecolor{intNodeColor}{rgb}{0.5,0.8,0.5}
\definecolor{intArrowColor}{rgb}{0.1,0.8,0.1}
\definecolor{boolNodeColor}{rgb}{0.8,0.5,0.5}
\definecolor{boolArrowColor}{rgb}{0.8,0.2,0.2}



\setlength\delimitershortfall{-1.5pt}



\newcommand{\picalign}{\vspace{1ex}\hspace{-3ex}}

\newcommand{\yh}{\hat y}
\newcommand{\Iff}{\Leftrightarrow}
\newcommand{\eu}{\mathrm{EU}}
\newcommand{\seq}{\bar}

\newcommand{\hist}[1]{o_{\le #1}a_{<#1}}
\newcommand{\althist}[1]{o'_{\le #1}a'_{<#1}}
\newcommand{\dohist}[1]{o_{\le #1} \mid do(a_{<#1})}
\newcommand{\epair}{\left(\EE^1[U^1;\pi],\EE^2[U^2;\pi]\right)}
\newcommand{\bool}{B}


\newcommand{\bred}[1]{{\color{red}{#1}}}


\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eqn}[1]{Equation~\ref{eqn:#1}}
\newcommand{\lem}[1]{Lemma~\ref{lem:#1}}
\newcommand{\thm}[1]{Theorem~\ref{thm:#1}}
\newcommand{\defn}[1]{Definition~\ref{defn:#1}}
\newcommand{\prop}[1]{Proposition~\ref{prop:#1}}
\newcommand{\cor}[1]{Corollary~\ref{cor:#1}}
\newcommand{\sect}[1]{Section~\ref{sec:#1}}




\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\image}{image}

%\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

%\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}


\title{Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making}

\author{Andrew Critch\\ Machine Intelligence Research Institute \\ UC Berkeley, Center for Human Compatible AI}


\begin{document}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
This paper derives a novel form for the objective function of a sequential decision-making system acting on behalf of multiple players with differing beliefs, and proves that any Pareto optimal policy must optimize an objective of that form.  Existing multi-objective reinforcement learning (MORL) objectives are shown to be suboptimal relative to the new form.

Concretely, consider two players with different priors and utility functions who may cooperate to build a machine that takes actions on their behalf.  A representation is needed for how much the machine's policy will prioritize each player's interests over time.  Assuming the players have reached common knowledge of their situation, this paper derives a recursion that any Pareto optimal policy must satisfy.  Two qualitative observations can be made from the recursion: the machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs.  Observation (2) represents a substantial divergence from na\"{i}ve linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing MORL algorithms), which is shown here to be inadequate for Pareto optimal sequential decision-making on behalf of players with different beliefs.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

It has been argued that the first AI systems with generally super-human cognitive abilities will play a pivotal decision-making role in directing the future of civilization \citep{bostrom2014superintelligence}. If that is the case, an important question will arise: \emph{Whose values will the first super-human AGI systems serve?}

The ``single player'' value alignment problem---that of aligning the value function of an AGI system with the values of single human, or a team of humans in close agreement with one another---is already a very difficult one and should not be swept under the rug; approaches like inverse reinforcement learning (IRL) \citep{russell1998learning} \citep{ng2000algorithms} \citep{abbeel2004apprenticeship} and cooperative inverse reinforcement learning (CIRL) \citep{hadfield2016cooperative} have only begun to address it.

With that problem in mind, consider two nations---allies or adversaries---who must decide whether to cooperate in the deployment of a very powerful and autonomous AI system.  If the two parties cannot reach sufficient agreement as to what policy a jointly owned AI system should follow, joint ownership may be less attractive than building separate AI systems, one for each party.  This could mean an arms race between nations competing under time pressure to develop ever more powerful militarized AI systems. Under such race conditions, everyone loses, as each nation is afforded less time to ensure the safety and value alignment of its own system. 

The purpose of this paper is to begin work on AI architectures that are highly amenable to joint ownership, to alleviate race conditions.  The main theorem is a novel form for sequential decision-making policies that are Pareto optimal according to players with differing beliefs, which is shown here to be more attractive---from the perspective of the negotiating players---than policies that do not account for differences in their beliefs.

\subsection{Related work}

This paper may be viewed as extending results in several fields:

\paragraph{Social choice theory.} The whole of social choice theory and voting theory may be viewed as an attempt to specify an agreeable formal policy to enact on behalf of a group.  Harsanyi's utility aggregation theorem \citep{harsanyi1980cardinal} suggests one form of solution: maximizing a linear combination of group members' utility functions.  The present work shows that this solution is inappropriate when players have different beliefs, and \thm{main} may be viewed as an extension of Harsanyi's form that accounts simultaneously for differing priors and the prospect of future observations.  Indeed, Harsanyi's form follows as a direct corollary of \thm{main} when players do share the same beliefs (\cor{harsanyi}).

\paragraph{Bargaining theory.} The formal theory of bargaining, as pioneered by Nash \citep{nash1950bargaining} and carried on by authors such as Myerson \citep{myerson1979incentive} \citep{myerson2013game} and Satterthwaite \citep{myerson1983efficient}, is also extremely topical.  Future investigation in this area might be aimed at generalizing their work to sequential decision-making settings, and this author recommends a focus on research specifically targeted at resolving conflicts.

\paragraph{Multi-agent systems.} There is ample literature examining multi-agent systems using sequential decision-making models.  \citet{shoham2008multiagent} survey various models of multiplayer games using an MDP to model each agent's objectives.  Chapter 9 of the same text surveys social choice theory, but does not account for sequential decision-making.  

\citet{zhang2014fairness} may be considered a sequential decision-making approach to social choice: they use MDPs to represent the decisions of players in a competitive game, and exhibit an algorithm for the players that, if followed, arrives at a Pareto optimal Nash equilibrium satisfying a certain fairness criterion.  Among the literature surveyed here, that paper is the closest to the present work in terms of its intended application: roughly speaking, achieving mutually desirable outcomes via sequential decision-making.  However, that work is concerned with an ongoing interaction between the players, rather than selecting a policy for a single agent to follow as in this paper.  

%The theory of Dec-POMPDs considers systems of sequential decision-making agents but in that setting, optimality is been characterized by a fixed, agreed-upon model of the environment \citep{oliehoek2008optimal}, and a shared objective.

\paragraph{Multi-objective sequential decision-making.} There is also a good deal of work on Multi-Objective Optimization (MOO)  \citep{tzeng2011multiple}, including for sequential decision-making, where solution methods have been called Multi-Objective Reinforcement Learning (MORL).  For instance, \citet{gabor1998multi} introduce a MORL method called Pareto Q-learning for learning a set of a Pareto optimal polices for a Multi-Objective MDP (MOMDP).  \citet{soh2011evolving} define Multi-Reward Partially Observable Markov Decision Processes (MR-POMDPs), and use use genetic algorithms to produce non-dominated sets of policies for them.  \citet{roijers2015point} refer to the same problems as Multi-objective POMDPS (MOPOMDPs), and provide a bounded approximation method for the optimal solution set for all possible weightings of the objectives.  \citet{wang2014multi} surveys MORL methods, and contributes Multi-Objective Monte-Carlo Tree Search (MOMCTS) for discovering multiple Pareto optimal solutions to a multi-objective optimization problem.   \citet{wray2015multi} introduce Lexicographic Partially Observable Markov Decision Process (LPOMDPs), along with two accompanying solution methods.

However, none of these or related works address scenarios where the objectives are derived from players with differing beliefs, from which the priority-shifting phenomenon of \thm{main} arises.  Differing beliefs are likely to play a key role in negotiations, so for that purpose, the formulation of multi-objective decision-making adopted here is preferable.

\section{Notation}

Random variables are denoted by uppercase letters, e.g., $S_1$, and lowercase letters, e.g., $s_1$, are used as indices ranging over the values of a variable, as in the equation
\[
\EE[S_1] = \sum_{s_1} \PP(s_1)\cdot s_1.
\]

Sequences are denoted by overbars, e.g., given a sequence $(s_1,\ldots,s_n)$, $\seq s$ stands for the whole sequence.   Subsequences are denoted
by subscripted inequalities, so e.g., $s_{<4}$ stands for $(s_1,s_2,s_3)$,
and $s_{\le 4}$ stands for $(s_1,s_2,s_3,s_4)$.

\section{Two players building a single agent}

Consider, informally, a scenario wherein two players---perhaps individuals, companies, or states---are considering cooperating to build or otherwise obtain a machine that will then interact with an environment on their behalf.\footnote{The results here all generalize from two players to $n$ players being combined successively in any order, but for clarity of exposition, the two person case is prioritized.} In such a scenario, the players will tend to bargain for ``how much'' the machine will prioritize their separate interests.  Therefore, to begin, we need some way to quantify ``how much'' each player is prioritized.

%\bred{Not ready to handle BATNAS yet in sequential decisions}
%Rational players would not choose to cooperate in such a scenario unless it could bring them more expected utility than acting unilaterally.  Thus, if the players have somewhat different objectives, some bargaining will be necessary for them to ensure the jointly owned machine acts in a way that is sufficiently favorable to each of them.  For example, if player 1 believes she could probably build a machine almost as cheaply on her own that would easily out-compete any efforts by player 2 to control the environment, then player $1$ will not be easily enticed to cooperate.  In this case, player 1 would argue that because of her high BATNA (best alternative to negotiated agreement), in order for cooperation to make sense, the cooperatively built machine would have to prioritize her highly in its actions.
For instance, one might model the machine as maximizing the expected value, given its observations, of some utility function $U$ of the environment that equals a weighted sum 
\begin{equation}\label{eqn:harsanyi}
w^1U^1 + w^2U^2
\end{equation}
of the players' individual utility functions $U^1$ and $U^2$, as Harsanyi's social aggregation theorem \citep{harsanyi1980cardinal} recommends.  Then the bargaining process could focus on choosing the values of the weights $w^i$.  

However, this turns out to be a suboptimal approach.  As we shall see in \prop{impossibility}, this solution form is not generally compatible with Pareto optimality when agents have different beliefs.  Harsanyi's setting does not account for agents having different priors, nor for decisions being made sequentially, after future observations.  In such a setting, we need a new form of solution, exhibited here along with a recursion that characterizes optimal solutions by a process analogous to, but meaningfully different from, Bayesian updating.

\subsection{A POMDP formulation}

Let us formalize the machine's decision-making situation using the structure of a Partially Observable Markov Decision Process (POMDP), as depicted by the Bayesian network in \fig{pomdp}.  (See \citet{russell2003artificial} for an introduction to POMDPs, and \citet{darwiche2009modeling} for an introduction to Bayesian networks.)

At each point in time $i$, the machine will have a policy $\pi_i$ that for each possible sequence of observations $o_{\le i}$ and past actions $a_{<i}$, returns a distribution $\pi_i(- \mid \hist i)$ on actions $a_i$, which will then be used to generate an action $a_i$ with probability $\pi(a_i \mid \hist i)$.  In \fig{pomdp}, the part of the Bayes net governed by the machine's policy is highlighted in green.

\begin{figure}
\pgfmathtruncatemacro{\len}{2}
\pgfmathtruncatemacro{\lenn}{\len + 1}
\pgfmathtruncatemacro{\lennn}{\len + 2}
\begin{center}
\begin{tikzpicture}[scale=0.8, every node/.style={transform shape}, node distance=1cm, auto, very thick, >=stealth']

  \foreach \x in {1,...,\lennn}{
        \node [draw, circle, fill=envNodeColor]  (s\x) at (3*\x,2) {$S_\x$};
        \node [color=envArrowColor, below left = -0.1cm and 0.3cm of s\x] (T\x) {};
        }

  \foreach \x in {1,...,\lenn}{
       \node [draw, circle, fill=intNodeColor]  (o\x) at (3*\x,0) {$O_\x$};
       \node [draw, circle, fill=intNodeColor]  (a\x) at (3*\x+1.4,0) {$A_\x$};
       }

  \foreach \x in {1,...,\lenn}{
      \pgfmathtruncatemacro{\xn}{\x +1 }
      \draw[->,envArrowColor] (s\x) to node [right] {} (o\x);
      \draw[->,intArrowColor] (o\x) to node {} (a\x);
      \draw[->,envArrowColor] (a\x) to node {} (s\xn);
      \draw[->,envArrowColor] (s\x)--(s\xn);
      }
      
  \foreach \x in {1,...,\len}{
      \pgfmathtruncatemacro{\xn}{\x +1 };
      \foreach \y in {\xn,...,\lenn}{
          \draw[->,intArrowColor] (o\x) to [out=330,in=240] node {} (a\y);
          \draw[->,intArrowColor] (a\x) to [out=330,in=240] node {} (a\y);
          }
      }
  
  \node [draw, circle, fill=envNodeColor] (u) at (3*\lennn,4) {$U$};

  \foreach \x in {1,...,\lennn}{
          \draw[->,envArrowColor] (s\x) to node {} (u);  
  }
  
  
\end{tikzpicture}
\end{center}
\vspace{-6ex}
\caption{A POMDP of length $n=\lenn$}
\label{fig:pomdp}
\end{figure}

\paragraph{Common knowledge assumptions.}  It is assumed that the players will have common knowledge of the policy $\pi = (\pi_1,\ldots,\pi_n)$ they select for the machine to implement, but that the players may have different beliefs about how the environment works, and of course different utility functions.  It is also assumed that the players have common knowledge of one another's posterior.  

\emph{This last assumption is critical.}  During a bargaining process, one should expect players' beliefs to update in response to one another's behavior.   Assuming common knowledge of posteriors means that the players have reached an equilibrium where, each knowing what the other believes, does not wish to further update her own beliefs.\footnote{It is enough to assume the players have reached a ``persistent disagreement'' that cannot be mediated by the machine in some way.  Future work should design solutions for facilitating the process of attaining common knowledge, or to obviate the need to assume it.}

We encode each player $j$'s outlook as a POMDP, $D^j = (\Ss^j,\Aa,T^j, U^j, \Oo, \Omega^j, n)$, which simultaneously represents that player's beliefs about the environment, and the player's utility function.
\begin{itemize}
\item $\Ss^j$ represents a set of possible states $s$ of the environment,
\item $\Aa$ represents the set of possible actions $a$ available to the machine,
\item $T^j$ represents the conditional probabilities player $j$ believes will govern the environment state transitions, i.e., $\PP^j(s_{i+1}\mid s_i a_i)$,
\item $U^j$ represents player $j$'s utility function from sequences of environmental states $(s_1,\ldots,s_n)$ to $\RR$; for the sake of generality, $U^j$ is \emph{not assumed} to be additive over time, as reward functions often are, 
\item $\Oo$ represents the set of possible observations $o$ of the machine, 
\item $\Omega^j$ represents the conditional probabilities player $j$ believes will govern the machine's observations, i.e., $\PP^j(o_i\mid s_i)$, and
\item $n$ is the number of time steps.
\end{itemize}
%
Thus, player $j$'s subjective probability of an outcome $(\seq s, \seq o, \seq a)$, for any $\seq s \in (\Ss^j)^n$, is given by a probability distribution $\PP^j$ that takes $\pi$ as a parameter:
%\begin{equation}
\begin{multline}
\label{eqn:pomdp}
\PP^j(\seq s, \seq o, \seq a ; \pi) := \PP^j(s_1) \cdot \prod_{i=1}^{n} 
\PP^j(o_i \mid s_i)\, \\  \pi(a_i \mid \hist i)\, \PP^j(s_{i+1} \mid s_ia_i)
\end{multline}
%\end{equation}

As such, the POMDPs $D^1$ and $D^2$ are ``compatible'' in the following sense:

\begin{definition}[Compatible POMDPs]  We say that two POMDPs, $D^1$ and $D^2$, are \emph{compatible} if any policy for one may be viewed as a policy for the other, i.e., they have the same set of actions $\Aa$ and observations $\Oo$, and the same number of time steps $n$.
\end{definition}

\subsection{Pareto optimal policies}

In this context, where a policy $\pi$ may be evaluated relative to more than one POMDP, we use superscripts to represent which POMDP is governing the probabilities and expectations, e.g.,
\[
\EE^j[U^j; \pi] := \sum_{\seq s \in (\Ss^j)^n} \PP^j(\bar s; \pi) U^j(\bar s)
\]
represents the expectation in $D^j$ of the utility function $U^j$, assuming policy $\pi$ is followed.
\begin{definition}[Pareto optimal policies]  A policy $\pi$ is \emph{Pareto optimal} for a compatible pair of POMDPs $(D^1,D^2)$ if for any other policy $\pi'$, either
\[
\EE^1[U^1;\pi] \ge \EE^1[U^1;\pi'] \textrm{\quad or \quad} \EE^2[U^2;\pi] \ge \EE^2[U^2;\pi'].
\]
\end{definition}
%
It is assumed that, during negotiation, the players will be seeking a Pareto optimal policy for the machine to follow, relative to the POMDPs $D^1$ and $D^2$ describing each player's outlook.

\paragraph{Policy mixing assumption.}  It is also assumed that during the agent's first action (or before it), the agent has the ability to generate and store some random numbers in the interval $[0,1]$, called a random seed, that will not affect the environment except through other features of its actions.  Then, given any two policies $\pi$ and $\pi'$ and a scalar $p\in[0,1]$ we may construct a third policy, 
\[
p\pi + (1-p)\pi',
\]
 that decides with probability $p$ (before receiving any inputs) to use policy $\pi$ for generating all of its future actions, and otherwise uses policy $\pi'$.  (This is a ``once and for all'' decision; the agent does not flip-flop between $\pi$ and $\pi'$ once the decision is made.)  Mixtures of more than two policies are defined similarly.  With this formalism, whenever $\sum_k \alpha_k = 1$ and each $\alpha_k\ge 0$, we have
\begin{equation}\label{eqn:linearity}
\EE^j\left[U^j ; \sum_k \alpha_k \pi_k\right] = \sum_k \alpha_k \EE^j[U^j ; \pi_k].
\end{equation}

\begin{lemma}\label{lem:pareto}
A policy $\pi$ is Pareto optimal to players $1$ and $2$ if and only if there exist weights $w^1,w^2\geq 0$ with $w_1+w_2=1$ such that
\begin{equation}\label{eqn:pareto}
\pi \in \argmax_{\pi^* \in\Pi}\left(w^1\EE^1[U^1;\pi^*] + w^2\EE^2[U^2; \pi^*]\right)
\end{equation}
\end{lemma}

\begin{proof}
The mixing assumption gives the space of policies $\Pi$ the structure of a convex space that the maps $\EE^j[U^j; -] $ respect by \eqn{linearity}.  This ensures that the image of the map $f:\Pi\to\RR^2$ given by
\[
f(\pi) := \left(\EE^1[U^1;\pi],\; \EE^2[U^2;\pi]\right)
\]
is a closed, convex polytope.  As such, a point $(x,y)$ lies on the Pareto boundary of $\image(f)$ if and only if there exist nonnegative weights $(w^1,w^2)$, not both zero, such that 
\[
(x,y) \in \argmax_{(x^*,y^*)\in \image(f)} \left(w^1x^* + w^2y^*\right)
\]
After normalizing $w^1+w^2$ to equal $1$, this implies the result.
\end{proof}

\subsection{A reprioritization mechanism that resembles Bayesian updating}

To help us interpret the conclusion of \lem{pareto}, for any weights, $w^1,w^2\ge 0$ with $w^1+w^2=1$, we define a new POMDP that works by flipping a $(w^1,w^2)$-weighted coin, and then running $D^1$ or $D^2$ thereafter, according to the coin flip.  Explicitly, 

\begin{definition}[POMDP mixtures]\label{defn:mixture} Suppose that $D^1$ and $D^2$ are compatible POMDPs, with parameters 
$D^j = (\Ss^j,\Aa,T^j, U^j, \Oo, \Omega^j, n)$.  Define a new POMDP compatible with both, denoted $D=w^1D^1 + w^2D^2$, with parameters 
$D^j = (\Ss,\Aa,T, U, \Oo, \Omega, n)$, as follows:
\begin{itemize}
\item $\Ss:= \{(j,s) \mid j\in\{1,2\}, s\in\Ss^j\}$,
\item Environmental transition probabilities $T$ given by
%
\begin{align*}
& \PP\left((j,s_1)\right) := \: w^j\cdot \PP^j(s_1)\\
\intertext{for any initial state $s_1\in\Ss^j$, and thereafter,}
& \PP\left((j',s_{i+1}) \mid (j,s_i), a_i \right) & \\
& := \;
\begin{cases}
\PP^j\left(s_{i+1} \mid s_ia_i\right) &\mbox{ if $j'=j$}\\
0 & \mbox { if $j'\neq j$}
\end{cases}
\end{align*}

Hence, the value of $j$ will be constant over time, so a full history for the environment may be represented by a pair
\[
(j,\seq s) \in \{1\}\times (\Ss^1)^n \cup \{2\}\times(\Ss^2)^n.
\]
Let $\bool $ denote the boolean random variable that equals whichever constant value of $j$ obtains, so then
\[
\PP(\bool =j) = w^j
\]
\item The utility function $U$ is given by
\[
U(j,\seq s) := U^j(\seq s)
\]
\item The observation probabilities $\Omega$ are given by
\[
\PP\left(o_i \mid (j,s_i)\right) := \PP(\bool =j) \cdot \PP^j(o_i \mid s_i)
\]
In particular, the policy does not observe directly whether $j=1$ or $j=2$.
\end{itemize}

\end{definition}
\begin{figure}
\pgfmathtruncatemacro{\len}{2}
\pgfmathtruncatemacro{\lenn}{\len + 1}
\pgfmathtruncatemacro{\lennn}{\len + 2}
\begin{center}
\begin{tikzpicture}[scale=0.8, every node/.style={transform shape}, node distance=1cm, auto, very thick, >=stealth']
        \node [draw, circle, fill=boolNodeColor]  (b) at (3,4) {$\bool $};
        
  \foreach \x in {1,...,\lennn}{
        \node [draw, circle, fill=envNodeColor]  (s\x) at (3*\x,2) {$S_\x$};
        \draw [->,boolArrowColor] (b) to node {} (s\x);
        \node [color=envArrowColor, below left = -0.1cm and 0.3cm of s\x] (T\x) {};
        }

  \foreach \x in {1,...,\lenn}{
       \node [draw, circle, fill=intNodeColor]  (o\x) at (3*\x,0) {$O_\x$};
       \draw [->,boolArrowColor] (b) to [out=270,in=145] node [right] {} (o\x);
       \node [draw, circle, fill=intNodeColor]  (a\x) at (3*\x+1.4,0) {$A_\x$};
       }

  \foreach \x in {1,...,\lenn}{
      \pgfmathtruncatemacro{\xn}{\x +1 }
      \draw[->,envArrowColor] (s\x) to node [right] {} (o\x);
      \draw[->,intArrowColor] (o\x) to node {} (a\x);
      \draw[->,envArrowColor] (a\x) to node {} (s\xn);
      \draw[->,envArrowColor] (s\x)--(s\xn);
      }
      
  \foreach \x in {1,...,\len}{
      \pgfmathtruncatemacro{\xn}{\x +1 };
      \foreach \y in {\xn,...,\lenn}{
          \draw[->,intArrowColor] (o\x) to [out=330,in=240] node {} (a\y);
          \draw[->,intArrowColor] (a\x) to [out=330,in=240] node {} (a\y);
          }
      }
  
  \node [draw, circle, fill=envNodeColor] (u) at (3*\lennn,4) {$U$};
  \draw[->,boolArrowColor] (b) to node [right] {} (u);

  \foreach \x in {1,...,\lennn}{
          \draw[->,envArrowColor] (s\x) to node {} (u);  
  }
\end{tikzpicture}
\end{center}
\vspace{-6ex}
\caption{A POMDP (mixture) of length $n=\lenn$ initialized by a Boolean $\bool $}
\label{fig:pomdp2}
\end{figure}

The POMDP mixture $D=w^1D^1 + w^2D^2$ can be depicted by a Bayes net by adding an additional environmental node for $\bool $ in the diagram of $D^1$ and $D^2$ (see \fig{pomdp2}).  Indeed, given any policy $\pi$, the expected payoff of $\pi$ in $w^1D^1+w^2D^2$ is exactly
\begin{multline*}
\; \PP(\bool =1)\cdot\EE(U \mid \bool =1 ; \pi) \\  + \PP(\bool =2)\cdot\EE(U \mid \bool =2 ; \pi)\\
= \; w^1\EE^2(U^1; \pi) + w^2\EE^2(U^2 ; \pi)
\end{multline*}
%
Therefore, using the above definitions, \lem{pareto} may be restated in the following equivalent form:

\begin{lemma}\label{lem:mixture}
Given a pair $(D^1,D^2)$ of compatible POMDPs, a policy $\pi$ is Pareto optimal for that pair if and only if there exist weights $w^j\geq 0$ with $w^1+w^2=1$ such that $\pi$ is an optimal policy for the single POMDP given by $w^1D^1+w^2D^2$.  
\end{lemma}

\subsection{A recursive Pareto optimality condition}

Expressed in the form of \eqn{pareto}, it might not be clear how a Pareto optimal policy makes use of its observations over time, aside from storing them in memory.  For example, is there any sense in which the machine carries ``beliefs" about the environment that it ``updates" at each time step?  \lem{mixture} allows us to reduce some such questions about Pareto optimal policies to questions about single POMDPs.  

If $\pi$ is an optimal policy for a single POMDP, at any time step $i$, optimality of the action distribution $\pi_i(-\mid\hist{i})$ can be characterized without reference to the previous policy components 
$(\pi_1,\ldots,\pi_{i-1})$, nor to $\pi_i(-\mid\althist{i})$ for any alternate history $\althist{i}$.  To express this claim in an equation, Pearl's ``$do()$" notation \citep{pearl2009causality} comes in handy:
\begin{definition}[``do'' notation]
\begin{multline*}
\PP^j(\seq o \mid do(\seq a)) \\ := \sum_{\seq s \in (\Ss^j)^n} \PP^j(s_1) \cdot \prod_{i=1}^n
\PP^j(o_i \mid s_i)\, \PP^j(s_{i+1} \mid s_ia_i)
\end{multline*}
\end{definition}

\begin{proposition}[Classical separability]\label{prop:separability}
If $D$ is a POMDP described by conditional probabilities $\PP(-\mid -)$ and utility function $U$ (as in \eqn{pomdp}), then a policy $\pi$ is optimal for $D$ if and only if for each time step $i$ and each observation/action history $\hist{i}$, the action distribution $\pi_i(-\mid \hist{n})$ satisfies the following backward recursion:
%OLD VERSION
%\begin{align*}
%& \pi_i(-\mid\hist{i}) \in \argmax_{\alpha\in\Delta A} \left(\vphantom{\left(()\right)}\right.\\
%& \left.\quad \PP(\dohist{i})\cdot \EE[U \mid \hist{i} ;\; a_n\sim \alpha; \; \pi_{i+1},\ldots,\pi_{n}]\right)
%\end{align*}
\begin{multline*}
 \pi_i(-\mid\hist{i}) \in \argmax_{\alpha\in\Delta A}\left( \PP(\dohist{i})\cdot \right. \\ \left.\EE[U \mid \hist{i} ;\; a_n\sim \alpha; \; \pi_{i+1},\ldots,\pi_{n}] \right)
\end{multline*}
This characterization of $\pi_i(\hist{i})$ does not refer to $\pi_1,\ldots,\pi_{i-1}$, nor to 
$\pi_i(\althist{i})$ for any alternate history $\althist{i}$.
\end{proposition}

\begin{proof}
This is just Bellman's optimality condition.
\end{proof}

It turns out that Pareto optimality can be characterized in a similar way by backward recursion from the final time step.  The resulting recursion reveals a pattern in how the weights on the players' conditionally expected utilities must change over time, which is the main result of this paper:

\begin{theorem}[Pareto optimal policy recursion]\label{thm:main}
Given a pair $(D^1,D^2)$ of compatible POMDPs of length $n$, a policy $\pi$ is Pareto optimal if and only if its components $\pi_i$ for $i\le n$ satisfy the following backward recursion for some pair of weights $w^1,w^2\geq 0$ with $w^1+w^2=1$:
%OLD VERSION
%\begin{align*}
%&\pi^i(-\mid\hist{i}) \in \argmax_{\alpha\in\Delta A} \left(\vphantom{\left(\left(\right)\right)}\right.\\
%& \left.w^1 \PP^1\left(\dohist{i}; \pi_{i+1},\ldots,\pi_n\right) \cdot \EE^1[U^1 \mid \hist{i} a_i ;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n]\right.\\
%+ & \left.w^2 \PP^2\left(\dohist{i}; \pi_{i+1},\ldots,\pi_n\right) \cdot \EE^2[U^2 \mid \hist{i} a_i;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n] \right)
%\end{align*}
\begin{align*}
\pi^i&(-\mid\hist{i}) \in \argmax_{\alpha\in\Delta A} \biggl(\vphantom{\left(\left(\right)\right)}\biggr.\\
 &\left.w^1 \PP^1\left(\dohist{i}\right)\right. \\ 
 & \quad \quad \left.\cdot \EE^1[U^1 \mid \hist{i} a_i ;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n]\right.\\
+ &\left.w^2 \PP^2\left(\dohist{i}\right)\right. \\ 
& \quad \quad \biggl.\cdot \EE^2[U^2 \mid \hist{i} a_i;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n] \biggr)
\end{align*}

In words, to achieve Pareto optimality, the machine must
\begin{enumerate}
\item use each player's own beliefs when estimating the degree to which a decision favors that player's utility function, and
\item shift the relative priorities of the players' expected utilities in the machine's decision objective over time, by a factor proportional to how well the players predict the machine's inputs.
\end{enumerate}
\end{theorem}

\begin{proof}
By \lem{mixture}, the Pareto optimality of $\pi$ for $(D^1,D^2)$ is equivalent to its classical optimality for
$w^1D^1 + w^2D^2$ for some $(w^1,w^2)$, which by \prop{separability} is equivalent to satisfying the following backward recursion (writing $\PP$ and $\EE$ for probabilities and expectations in $w^1D^1 + w^2D^2$):
%OLD VERSION
%\begin{align*}
%&\pi^i(-\mid\hist{i}) \in \argmax_{\alpha\in\Delta A} \left(\vphantom{\left(\left(\right)\right)}\right.\\
%& \left.\PP(\bool =1)\cdot \PP\left(\dohist{i}; \pi_{i+1},\ldots,\pi_n\right)\cdot \EE[U \mid \hist{i} a_i ;\; a_i\sim %\alpha;\; \pi_{i+1},\ldots,\pi_n]\right.\\
%+ & \left.\PP(\bool =2)\cdot \PP\left(\dohist{i}; \pi_{i+1},\ldots,\pi_n\right)\cdot \EE[U \mid \hist{i} a_i;\; a_i\sim %\alpha;\; \pi_{i+1},\ldots,\pi_n] \right).
%\end{align*}
\begin{align*}
&\pi^i(-\mid\hist{i}) \in \argmax_{\alpha\in\Delta A} \biggl(\vphantom{\left(\left(\right)\right)}\biggr.\\
& \left.\PP(\bool =1)\cdot \PP\left(\dohist{i}\right)\right. \\
 & \quad \quad \left.\cdot \EE[U \mid \hist{i} a_i ;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n]\right.\\
+ & \left.\PP(\bool =2)\cdot \PP\left(\dohist{i}\right)\right. \\
 & \quad \quad \biggl.\cdot \EE[U \mid \hist{i} a_i;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n] \biggr).
\end{align*}
By \defn{mixture}, the expression inside the $\argmax$ equals 
%OLD VERSION
%\begin{align*}
%& w^1 \PP^1\left(\dohist{i}; \pi_{i+1},\ldots,\pi_n\right)\cdot \EE^1[U^1 \mid \hist{i} a_i ;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n]\\
%+ & w^2 \PP^2\left(\dohist{i}; \pi_{i+1},\ldots,\pi_n\right)\cdot \EE^2[U^2 \mid \hist{i} a_i;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n]
%\end{align*}
\begin{align*}
& w^1 \PP^1\left(\dohist{i}\right) \\
& \quad \quad \cdot \EE^1[U^1 \mid \hist{i} a_i ;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n]\\
+ & w^2 \PP^2\left(\dohist{i}\right) \\
& \quad \quad \cdot \EE^2[U^2 \mid \hist{i} a_i;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n]
\end{align*}
hence the result.
\end{proof}

When the players have the same beliefs, they aways assign the same probability to the machine's inputs, so the weights on their respective expectations do not change over time.  In this case, Harsanyi's utility aggregation formula is recovered as a special instance:

\begin{corollary}[Harsanyi's utility aggregation formula]\label{cor:harsanyi}
Suppose that players 1 and 2 share the same beliefs about the environment, i.e., the pair $(D^1,D^2)$ of compatible POMDPs agree on all parameters except the players' utility functions $U^1\neq U^2$. Then a policy $\pi$ is Pareto optimal if and only if there exist weights $w^1,w^2\geq 0$ with $w^1+w^2=1$ such that for $i\le n$, $\pi_i$ satisfies
\begin{multline*}
\pi^i(-\mid\hist{i}) \in \argmax_{\alpha\in\Delta A} \left(\EE[w^1U^1+w^2U^2]\right. \\
\left. \mid \hist{i} a_i ;\; a_i\sim \alpha;\; \pi_{i+1},\ldots,\pi_n]\right)
\end{multline*}
where $\EE=\EE^1=\EE^2$ denotes the shared expectations of both players.
\end{corollary}
\begin{proof}
Setting $\EE=\EE^1=\EE^2$ in \thm{main}, factoring out the common coefficient $\PP^1\left(\dohist{i}; \pi_{i+1},\ldots,\pi_n\right)=\PP^2\left(\dohist{i}; \pi_{i+1},\ldots,\pi_n\right)$, and applying linearity of expectation yields the result.
\end{proof}

\subsection{Comparison to na\"{i}ve utility aggregation}\label{sec:comparison}

To see the necessity of the $\PP^j$ terms that shift the expectation weights in \thm{main} over time, let us compare it with the behavior of an alternative optimization criterion that maximizes a fixed linear combination of expectations.

\paragraph{A cake-splitting scenario.}  The parameters of this scenario are laid out in Table \ref{table:scenario}, and described as follows:

Alice (Player 1) and Bob (Player 2) are about to be presented with a cake which they can choose to split in half to share, or give entirely to one of them.  They have (built or purchased) a robot that will make the cake-splitting decision on their behalf.  Alice's utility function returns $0$ if she gets no cake, $20$ if she gets half a cake, or $30$ if she gets a whole cake.  Bob's utility function works similarly.

\begin{table*}[t]
\centering
\begin{tabular}{*{7}{|c}|}
\hline
$S_1$ & $O_1$& $\PP^1(O_1\mid S_1)$ & $\PP^2(O_1\mid S_1)$ & $A_1=S_1$ & $U^1$ & $U^2$ \\ \hline
\multirow{ 6}{*}{cake}
& \multirow{ 3}{*}{red}
& \multirow{ 3}{*}{$90\%$}
& \multirow{ 3}{*}{$10\%$}
& (all, none) & 30 & 0 \\ 
&&&& (half, half) & 20 & 20 \\ 
&&&& (none, all) & 0 & 30 \\ 
\cline{2-7}
& \multirow{ 3}{*}{green}
& \multirow{ 3}{*}{$10\%$}
& \multirow{ 3}{*}{$90\%$}
& (all, none) & 30 & 0 \\ 
&&&& (half, half) & 20 & 20 \\ 
&&&& (none, all) & 0 & 30 \\ 
\hline
\end{tabular}
\caption{An example scenario wherein a Pareto optimal policy undergoes priority shifting}
\label{table:scenario}
\end{table*}

However, Alice and Bob have slightly different beliefs about how the environment works.  They both agree on the state of the environment that the robot will encounter at first: a room with a cake in it ($S_1=\text{``cake''}$).  But Alice and Bob have different predictions about how the robot's sensors will perceive the cake: Alice thinks that when the robot perceives the cake, it is $90\%$ likely to appear with a red tint ($O_1=\text{``red"}$), and $10\%$ likely to appear with a green tint ($O_1=\text{``green"}$), whereas Bob believes the exact opposite.  In either case, upon seeing the cake, the robot will either give Alice the entire cake ($A_1=S_1=\text{(all, none)}$), split the cake half-and-half ($A_1=S_1=\text{(half, half)}$), or give Bob the entire cake ($A_1=S_1=\text{(none, all)}$).  Moreover, Alice and Bob have common knowledge of all these facts.
%FORMER PLACEMENT OF TABLE

Now, consider the following Pareto optimal policy that favors Alice (Player 1) when $O_1$ is red, and Bob (Player 2) when $O_1$ is green:
%
\begin{align*}
&\hat\pi(- \mid \text{red}) = 100\%\text{(all, none)}\\
&\hat\pi(- \mid \text{green}) = 100\%\text{(none, all)}
\end{align*}
%
This policy can be viewed intuitively as a bet between Alice and Bob about the value of $O_1$, and is highly appealing to both players:
%
\begin{align*}
\EE^1[U^1; \hat\pi] &= 90\%(30) + 10\%(0) = 27\\
\EE^2[U^2; \hat\pi] &= 10\%(0) + 90\%(30) = 27
\end{align*}
In particular, $\hat\pi$ is more appealing to both Alice and Bob than an agreement to deterministically split the cake (half, half).  However, 

\begin{proposition}\label{prop:impossibility}
The Pareto optimal strategy $\hat\pi$ above cannot be implemented by any machine that na\"{i}vely maximizes a fixed-over-time linear combination of the conditionally expected utility of the two players, i.e., by any policy $\pi$ satisfying 
%
\begin{multline}\label{eqn:naive}
\pi(- \mid o_1) \in \argmax_{\alpha\in\Delta A}\left(r\cdot\EE^1[U^1 \mid o_1; a_1\sim\alpha] + (1-r)\right. \\ 
\left. \cdot\EE^2[U^2 \mid o_1; a_1\sim\alpha]\right)
\end{multline}
%
for some fixed $r\in[0,1]$.  Moreover, every such policy $\pi$ is strictly worse than $\hat\pi$ in expectation to one of the players.
\end{proposition} 

This proposition is relatively unsurprising when one considers the policy $\hat\pi$ intuitively as a bet-settling mechanism, and that the nature of betting is to favor different preferences based on future observations.  However, to be sure of this impossibility claim, one must rule out the possibility that the $\hat\pi$ could be implemented by having the machine choose which element of the $\argmax$ in \eqn{naive} to use based on whether the cake appears red or green.  

\begin{proof}[Proof of \prop{impossibility}] Suppose $\pi$ is any policy satisfying \eqn{naive} for some fixed $r$, and consider the following cases for $r$:
\begin{enumerate}
\item If $r < 1/3 $, then $\pi$ must satisfy
\[
\pi(-\mid o_1) = 100\%\text{(none, all)}.
\] 
Here, $\EE^1[U^1 ; \pi] = 0 < 27$, so $\pi$ is strictly worse than $\hat\pi$ in expectation to Alice.

\item If $r = 1/3 $, then $\pi$ must satisfy
\[
\pi(-\mid o_1) = q(o_1)\text{(none, all)} + (1-q(o_1))\text{(half, half)}
\] 
for some $q(o_1)\in[0,1]$ depending on $o_1$.  Here, $\EE^1[U^1 ; \pi] \le 20 < 27$ (with equality when $q(\text{red})=q(\text{green})=1$), so $\pi$ is strictly worse than $\hat\pi$ in expectation to Alice.

\item If $1/3 < r < 2/3 $, then $\pi$ must satisfy
\[
\pi(-\mid o_1)=100\%\text{(half, half)}
\]
Here, $\EE^1[U^1 ; \pi] = \EE^2[U^2 ; \pi] = 20 < 27$, so $\pi$ is strictly worse than $\hat\pi$ in expectation to both Alice and Bob.
\end{enumerate}
The remaining cases, $r=2/3$ and  $r>2/3$, are symmetric to the first two, with Bob in place of Alice and (none, all) in place of (all, none).
\end{proof}

\section{Conclusion}

Insofar as \thm{main} is not particularly mathematically sophisticated---it employs only basic facts about convexity and linear algebra---this suggests there may be more low-hanging fruit to be found in the domain of ``machine implementable social choice theory".  To recapitulate, \thm{main} represents two deviations from the intuition of na\"{i}ve utility aggregation: to achieve Pareto optimality for players with differing beliefs, a machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs.

Future work should address methods for targeting a specific pair of expected utilities on behalf of the players, so as to more precisely facilitate bargaining.  As well, methods for helping the players to share information---perhaps in exchange for adjustments to the weights in \thm{main}---will be needed to help the players reach either a state of agreement or persistent disagreement that allows the theorem to be applied.  More ambitiously, bargaining models that account for a degree of transparency between the players should be employed, as individual humans and institutions have some capacity for detecting one another's intentions.

As a final remark, consider that social choice theory and bargaining theory were both pioneered during the Cold War, when it was particularly compelling to understand the potential for cooperation between human institutions that might behave competitively.  In the coming decades, machine intelligences will likely bring many new challenges for cooperation, as well as new means to cooperate, and new reasons to do so.  As such, new technical aspects of social choice and bargaining, along the lines of this paper, will likely continue to emerge.
%\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{APPENDIX}

%Appendixes should appear before the acknowledgment.

%\section*{Acknoweldgements}
%I am grateful to Stuart Russell and Dylan Hadfield-Menell, whose influence lead to me to think more about sequential decision-making, and to Sam Eisenstat, whose initial reading recommendations on bargaining got me off to a good start.  I also thank Anna Salamon for being supportive of my desire to enable collaborative outcomes in AI development.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%% The file named.bst is a bibliography style file for BibTeX 0.99c
\newpage
\bibliographystyle{named}
\bibliography{General,Inbox,MIRIPublications}

\end{document}
